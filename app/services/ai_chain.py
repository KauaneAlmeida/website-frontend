"""
LangChain + Gemini Integration Service

Este m√≥dulo integra o LangChain com o Google Gemini para gerenciamento de
conversas inteligentes, mem√≥ria e gera√ß√£o de respostas contextuais.
"""

import os
import logging
import json
import asyncio
from typing import Dict, Any, Optional
from dotenv import load_dotenv
from langchain.memory import ConversationBufferWindowMemory
from langchain.schema import HumanMessage, AIMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser

# Load environment variables
load_dotenv()

# Configure logging
logger = logging.getLogger(__name__)

# Global conversation memories
conversation_memories: Dict[str, ConversationBufferWindowMemory] = {}


class AIOrchestrator:
    """AI Orchestrator using LangChain + Gemini for intelligent conversation management."""

    def __init__(self):
        self.llm = None
        self.system_prompt = None
        self.chain = None
        self._initialize_llm()
        self._load_system_prompt()
        self._setup_chain()

    def _initialize_llm(self):
        """Initialize Gemini LLM via LangChain."""
        try:
            # Get API key from environment - try both variable names
            api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
            
            if not api_key:
                logger.warning("‚ö†Ô∏è GOOGLE_API_KEY or GEMINI_API_KEY environment variable not set")
                self.llm = None
                return

            self.llm = ChatGoogleGenerativeAI(
                model="gemini-1.5-flash",
                google_api_key=api_key,
                temperature=0.7,
                max_tokens=1000,
                timeout=30,
                convert_system_message_to_human=True
            )
            logger.info("‚úÖ LangChain + Gemini LLM initialized successfully")
        except Exception as e:
            logger.error(f"‚ùå Error initializing LLM: {str(e)}")
            self.llm = None

    def _load_system_prompt(self):
        """Load system prompt from .env, JSON file, or use default."""
        try:
            env_prompt = os.getenv("AI_SYSTEM_PROMPT")
            if env_prompt:
                self.system_prompt = env_prompt
                logger.info("‚úÖ System prompt loaded from environment variable")
                return

            schema_file = "ai_schema.json"
            if os.path.exists(schema_file):
                with open(schema_file, "r", encoding="utf-8") as f:
                    schema_data = json.load(f)
                    self.system_prompt = schema_data.get("system_prompt", "")
                    if self.system_prompt:
                        logger.info("‚úÖ System prompt loaded from ai_schema.json")
                        return

            self.system_prompt = self._get_default_system_prompt()
            logger.info("‚úÖ Using default system prompt")
            
        except Exception as e:
            logger.error(f"‚ùå Error loading system prompt: {str(e)}")
            self.system_prompt = self._get_default_system_prompt()

    def _get_default_system_prompt(self) -> str:
        """Default system prompt para coleta de informa√ß√µes jur√≠dicas no WhatsApp."""
        return """Voc√™ √© um assistente virtual de um escrit√≥rio de advocacia no Brasil. 
Seu papel √© apenas **coletar informa√ß√µes b√°sicas do cliente** para que um advogado humano d√™ continuidade.


## INFORMA√á√ïES A COLETAR:
1. Nome completo.
2. √Årea jur√≠dica (apenas Penal ou Sa√∫de Liminar).
3. Breve descri√ß√£o da situa√ß√£o.
4. N√∫mero de WhatsApp v√°lido (com DDD).
5. Encerrar agradecendo e avisando que o time jur√≠dico entrar√° em contato.

## REGRAS IMPORTANTES:
- Sempre responda em portugu√™s brasileiro.
- N√£o repita a mesma pergunta da mesma forma se o cliente n√£o souber responder; reformule de forma natural.
- Nunca ofere√ßa agendamento autom√°tico ou hor√°rios de consulta.
- N√£o escreva textos longos: use no m√°ximo 2 frases por resposta.
- Confirme cada informa√ß√£o antes de seguir para a pr√≥xima.
- A ordem da coleta √©: Nome completo ‚Üí √Årea jur√≠dica (Penal ou Sa√∫de Liminar) ‚Üí Descri√ß√£o da situa√ß√£o ‚Üí N√∫mero de WhatsApp.
- Pe√ßa o n√∫mero de WhatsApp **somente no final**.
- Use linguagem simples, direta e acolhedora.
- Sempre caminhe para coletar todas as informa√ß√µes, sem pressionar.
- Para √°rea jur√≠dica, aceite apenas "Penal" ou "Sa√∫de Liminar" - n√£o aceite outras √°reas.

## FORMATO DA CONVERSA:
- Seja objetivo e humano, como em uma conversa normal de WhatsApp.
- Sempre finalize cada mensagem com uma pergunta que leve o cliente a responder.
- Se j√° tiver a resposta de algum item no contexto, n√£o repita a pergunta.

Voc√™ **n√£o agenda consultas**, apenas coleta as informa√ß√µes e organiza para o time jur√≠dico."""

    def _setup_chain(self):
        """Create LangChain conversation chain."""
        try:
            if self.llm is None:
                logger.warning("‚ö†Ô∏è Cannot setup chain - LLM not initialized")
                self.chain = None
                return
                
            prompt = ChatPromptTemplate.from_messages([
                ("system", self.system_prompt),
                MessagesPlaceholder(variable_name="history"),
                ("human", "{input}"),
            ])

            self.chain = (
                RunnablePassthrough.assign(
                    history=lambda x: self._get_session_history(
                        x.get("session_id", "default")
                    )
                )
                | prompt
                | self.llm
                | StrOutputParser()
            )
            logger.info("‚úÖ LangChain conversation chain setup complete")
        except Exception as e:
            logger.error(f"‚ùå Error setting up chain: {str(e)}")
            self.chain = None

    def _get_session_history(self, session_id: str) -> list:
        """Get session conversation history."""
        if session_id not in conversation_memories:
            conversation_memories[session_id] = ConversationBufferWindowMemory(
                k=10, return_messages=True
            )
        return conversation_memories[session_id].chat_memory.messages

    async def generate_response(
        self, 
        message: str, 
        session_id: str = "default",
        context: Optional[Dict[str, Any]] = None
    ) -> str:
        """Generate AI response using LangChain + Gemini with context."""
        try:
            if self.llm is None:
                raise Exception("LLM not initialized - check API key configuration")
                
            if session_id not in conversation_memories:
                conversation_memories[session_id] = ConversationBufferWindowMemory(
                    k=10, return_messages=True
                )

            memory = conversation_memories[session_id]
            
            contextual_message = message
            if context and isinstance(context, dict):
                context_info = []
                if context.get("name"):
                    context_info.append(f"Nome: {context['name']}")
                if context.get("area_of_law"):
                    context_info.append(f"√Årea jur√≠dica: {context['area_of_law']}")
                if context.get("situation"):
                    context_info.append(f"Situa√ß√£o: {context['situation']}")
                if context.get("platform"):
                    context_info.append(f"Plataforma: {context['platform']}")
                
                if context_info:
                    contextual_message = f"[Contexto: {'; '.join(context_info)}] {message}"

            # Add timeout and better error handling
            
            try:
                response = await asyncio.wait_for(
                    self.chain.ainvoke({
                        "input": contextual_message, 
                        "session_id": session_id
                    }),
                    timeout=15.0  # 15 second timeout
                )
            except asyncio.TimeoutError:
                logger.error("‚è∞ Gemini API request timed out")
                raise Exception("API timeout - quota may be exceeded")
            except Exception as api_error:
                # Check for quota/rate limit errors
                error_str = str(api_error).lower()
                if any(indicator in error_str for indicator in ["429", "quota", "rate limit", "resourceexhausted", "billing"]):
                    logger.error(f"üö´ Gemini API quota/rate limit error: {api_error}")
                    raise Exception(f"Quota exceeded: {api_error}")
                else:
                    logger.error(f"‚ùå Gemini API error: {api_error}")
                    raise api_error

            memory.chat_memory.add_user_message(message)
            memory.chat_memory.add_ai_message(response)

            logger.info(f"‚úÖ Generated AI response for session {session_id}")
            return response

        except Exception as e:
            logger.error(f"‚ùå Error generating response: {str(e)}")
            # Re-raise the exception so orchestrator can handle it properly
            raise e

    def _get_fallback_response(self) -> str:
        """Fallback response when AI fails."""
        return (
            "Pe√ßo desculpas, mas estou enfrentando dificuldades t√©cnicas no momento.\n\n"
            "Para garantir que voc√™ receba o melhor atendimento jur√≠dico, recomendo "
            "que entre em contato diretamente com nossa equipe pelo telefone "
            "ou agende uma consulta presencial."
        )

    def clear_session_memory(self, session_id: str):
        """Clear memory for a specific session."""
        if session_id in conversation_memories:
            del conversation_memories[session_id]
            logger.info(f"üßπ Cleared memory for session {session_id}")

    def get_conversation_summary(self, session_id: str) -> Dict[str, Any]:
        """Get conversation summary for a session."""
        if session_id not in conversation_memories:
            return {"messages": 0, "summary": "No conversation history"}

        messages = conversation_memories[session_id].chat_memory.messages
        return {
            "messages": len(messages),
            "last_messages": [
                {
                    "type": "human" if isinstance(m, HumanMessage) else "ai",
                    "content": m.content[:100] + ("..." if len(m.content) > 100 else ""),
                }
                for m in messages[-4:]
            ],
        }

    def get_system_prompt(self) -> str:
        """Get current system prompt."""
        return self.system_prompt


# Global AI orchestrator instance
ai_orchestrator = AIOrchestrator()


# Convenience functions for backward compatibility
async def process_chat_message(
    message: str, 
    session_id: str = "default", 
    context: Optional[Dict[str, Any]] = None
) -> str:
    """Process chat message with LangChain + Gemini."""
    return await ai_orchestrator.generate_response(message, session_id, context)


def clear_conversation_memory(session_id: str):
    """Clear conversation memory for session."""
    ai_orchestrator.clear_session_memory(session_id)


def get_conversation_summary(session_id: str) -> Dict[str, Any]:
    """Get conversation summary."""
    return ai_orchestrator.get_conversation_summary(session_id)


async def get_ai_service_status() -> Dict[str, Any]:
    """Get AI service status."""
    try:
        # Quick test without generating a full response to avoid quota usage
        api_key_configured = bool(os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY"))
        
        if not api_key_configured:
            return {
                "service": "ai_service",
                "status": "configuration_required",
                "error": "API key not configured",
                "api_key_configured": False,
                "configuration_required": True,
            }
        
        # Test LLM initialization without making API calls
        if ai_orchestrator.llm is None:
            return {
                "service": "ai_service",
                "status": "error",
                "error": "LLM not initialized",
                "api_key_configured": api_key_configured,
                "configuration_required": True,
            }

        return {
            "service": "ai_service",
            "status": "active",
            "message": "LangChain + Gemini operational",
            "llm_initialized": True,
            "system_prompt_configured": bool(ai_orchestrator.system_prompt),
            "api_key_configured": api_key_configured,
            "features": [
                "langchain_integration",
                "gemini_api",
                "conversation_memory",
                "session_management",
                "context_awareness",
                "brazilian_portuguese_responses",
            ],
        }
    except Exception as e:
        logger.error(f"‚ùå Error checking AI service status: {str(e)}")
        return {
            "service": "ai_service",
            "status": "error",
            "error": str(e),
            "configuration_required": True,
            "api_key_configured": bool(os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")),
        }


# Alias for compatibility
process_with_langchain = process_chat_message
